#!/usr/bin/env bash
#
# tusk — single entry point for a project's task database.
#
# Usage:
#   tusk init [--force]        Create the DB with schema + triggers from config
#   tusk path                  Print the resolved DB path
#   tusk "SELECT ..."          Run a SQL statement
#   tusk -header -column "SQL" Pass sqlite3 flags + SQL
#   tusk shell                 Open an interactive sqlite3 shell
#   tusk config                Print full config JSON
#   tusk config <key>          Print config values (domains, task_types, agents, etc.)
#   tusk validate              Validate config.json against the expected schema
#   tusk sql-quote "text"      Escape and quote a string for safe SQL interpolation
#   tusk dupes check "..."     Check a summary for duplicates
#   tusk dupes scan            Find all duplicate pairs among open tasks
#   tusk dupes similar <id>    Find tasks similar to a given ID
#   tusk session-stats <id>    Populate token/cost stats for a session
#   tusk dashboard             Generate and open an HTML dashboard
#   tusk version               Print the installed tusk version
#   tusk migrate               Apply pending schema migrations
#   tusk regen-triggers        Drop and recreate validation triggers from config
#   tusk upgrade               Upgrade tusk from GitHub
#
# Configuration:
#   Project config:  <repo_root>/tusk/config.json
#   Fallback:        <install_dir>/config.default.json
#
# The DB path is defined here. Everything else reads it from this script.

set -euo pipefail

# ── Resolve paths ────────────────────────────────────────────────────

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
# Install dir is parent of bin/
INSTALL_DIR="$(cd "$SCRIPT_DIR/.." && pwd)"

find_repo_root() {
  local dir="$PWD"
  while [[ "$dir" != "/" ]]; do
    [[ -d "$dir/.git" ]] && echo "$dir" && return
    dir="$(dirname "$dir")"
  done
  echo "$PWD"
}

REPO_ROOT="$(find_repo_root)"
DB_DIR="$REPO_ROOT/tusk"
DB_PATH="$DB_DIR/tasks.db"
PROJECT_CONFIG="$DB_DIR/config.json"
# Default config: check sibling (installed) then parent dir (source repo)
if [[ -f "$SCRIPT_DIR/config.default.json" ]]; then
  DEFAULT_CONFIG="$SCRIPT_DIR/config.default.json"
else
  DEFAULT_CONFIG="$INSTALL_DIR/config.default.json"
fi

# ── Version ──────────────────────────────────────────────────────────

TUSK_GITHUB_REPO="gioe/tusk"

if [[ -f "$SCRIPT_DIR/VERSION" ]]; then
  TUSK_VERSION="$(cat "$SCRIPT_DIR/VERSION")"
elif [[ -f "$INSTALL_DIR/VERSION" ]]; then
  TUSK_VERSION="$(cat "$INSTALL_DIR/VERSION")"
else
  TUSK_VERSION="0"
fi

# ── Config helpers ───────────────────────────────────────────────────

resolve_config() {
  if [[ -f "$PROJECT_CONFIG" ]]; then
    echo "$PROJECT_CONFIG"
  elif [[ -f "$DEFAULT_CONFIG" ]]; then
    echo "$DEFAULT_CONFIG"
  else
    echo >&2 "Error: No config found at $PROJECT_CONFIG or $DEFAULT_CONFIG"
    exit 2
  fi
}

read_config_key() {
  local key="$1"
  local config
  config="$(resolve_config)"
  python3 -c "
import json, sys
with open(sys.argv[1]) as f:
    cfg = json.load(f)
val = cfg.get(sys.argv[2])
if val is None:
    sys.exit(1)
if isinstance(val, list):
    print('\n'.join(str(v) for v in val))
elif isinstance(val, dict):
    print(json.dumps(val, indent=2))
else:
    print(val)
" "$config" "$key"
}

read_config_json() {
  local config
  config="$(resolve_config)"
  cat "$config"
}

validate_config() {
  local config
  config="$(resolve_config)"
  python3 -c "
import json, sys

config_path = sys.argv[1]

# ── Load JSON ──
try:
    with open(config_path) as f:
        cfg = json.load(f)
except json.JSONDecodeError as e:
    print(f'Error: {config_path} is not valid JSON.', file=sys.stderr)
    print(f'  {e}', file=sys.stderr)
    sys.exit(1)

if not isinstance(cfg, dict):
    print(f'Error: {config_path} must be a JSON object (got {type(cfg).__name__}).', file=sys.stderr)
    sys.exit(1)

errors = []

# ── Check for unknown top-level keys ──
KNOWN_KEYS = {'domains', 'task_types', 'statuses', 'priorities', 'closed_reasons', 'agents', 'dupes'}
known_list = ', '.join(sorted(KNOWN_KEYS))
unknown = set(cfg.keys()) - KNOWN_KEYS
if unknown:
    for k in sorted(unknown):
        errors.append(f'Unknown config key \"{k}\". Valid keys: {known_list}')

# ── Validate list-of-strings fields ──
LIST_FIELDS = {
    'domains':        {'required': False},
    'task_types':     {'required': False},
    'statuses':       {'required': True},
    'priorities':     {'required': True},
    'closed_reasons': {'required': True},
}
for field, opts in LIST_FIELDS.items():
    if field not in cfg:
        if opts['required']:
            errors.append(f'Missing required key \"{field}\".')
        continue
    val = cfg[field]
    if not isinstance(val, list):
        errors.append(f'\"{field}\" must be a list (got {type(val).__name__}).')
        continue
    if opts['required'] and len(val) == 0:
        errors.append(f'\"{field}\" must not be empty.')
    for i, item in enumerate(val):
        if not isinstance(item, str):
            errors.append(f'\"{field}[{i}]\" must be a string (got {type(item).__name__}: {item!r}).')

# ── Validate agents (dict of string→string) ──
if 'agents' in cfg:
    agents = cfg['agents']
    if not isinstance(agents, dict):
        errors.append(f'\"agents\" must be an object (got {type(agents).__name__}).')
    else:
        for k, v in agents.items():
            if not isinstance(v, str):
                errors.append(f'\"agents.{k}\" value must be a string (got {type(v).__name__}: {v!r}).')

# ── Validate dupes (object with specific sub-keys) ──
if 'dupes' in cfg:
    dupes = cfg['dupes']
    if not isinstance(dupes, dict):
        errors.append(f'\"dupes\" must be an object (got {type(dupes).__name__}).')
    else:
        KNOWN_DUPE_KEYS = {'strip_prefixes', 'check_threshold', 'similar_threshold'}
        known_dupe_list = ', '.join(sorted(KNOWN_DUPE_KEYS))
        unknown_dupe = set(dupes.keys()) - KNOWN_DUPE_KEYS
        if unknown_dupe:
            for k in sorted(unknown_dupe):
                errors.append(f'Unknown key \"dupes.{k}\". Valid dupes keys: {known_dupe_list}')

        if 'strip_prefixes' in dupes:
            sp = dupes['strip_prefixes']
            if not isinstance(sp, list):
                errors.append(f'\"dupes.strip_prefixes\" must be a list (got {type(sp).__name__}).')
            else:
                for i, item in enumerate(sp):
                    if not isinstance(item, str):
                        errors.append(f'\"dupes.strip_prefixes[{i}]\" must be a string (got {type(item).__name__}: {item!r}).')

        for thresh in ('check_threshold', 'similar_threshold'):
            if thresh in dupes:
                tv = dupes[thresh]
                if not isinstance(tv, (int, float)):
                    errors.append(f'\"dupes.{thresh}\" must be a number (got {type(tv).__name__}: {tv!r}).')
                elif not (0 <= tv <= 1):
                    errors.append(f'\"dupes.{thresh}\" must be between 0 and 1 (got {tv}).')

# ── Report ──
if errors:
    print(f'Config validation failed ({config_path}):', file=sys.stderr)
    for e in errors:
        print(f'  - {e}', file=sys.stderr)
    sys.exit(1)
" "$config"
}

# ── SQL quoting helper ────────────────────────────────────────────────

sql_quote() {
  # Escapes a string for safe use in SQLite SQL statements.
  # Doubles internal single quotes and wraps the result in single quotes.
  #   sql_quote "O'Reilly"  →  'O''Reilly'
  #   sql_quote ""          →  ''
  # For NULL values, use the literal NULL (unquoted) — don't pass through sql_quote.
  local str="${1:-}"
  printf "'%s'" "$(printf '%s' "$str" | sed "s/'/''/g")"
}

# ── Generate trigger SQL from config ─────────────────────────────────

generate_triggers() {
  local config
  config="$(resolve_config)"
  python3 -c "
import json, sys

with open(sys.argv[1]) as f:
    cfg = json.load(f)

def trigger_sql(column, values):
    if not values:
        return ''
    quoted = ', '.join(f\"'{v}'\" for v in values)
    label = ', '.join(values)
    return f'''
CREATE TRIGGER validate_{column}_insert
BEFORE INSERT ON tasks FOR EACH ROW
WHEN NEW.{column} IS NOT NULL AND NEW.{column} NOT IN ({quoted})
BEGIN SELECT RAISE(ABORT, 'Invalid {column}. Must be one of: {label}'); END;

CREATE TRIGGER validate_{column}_update
BEFORE UPDATE OF {column} ON tasks FOR EACH ROW
WHEN NEW.{column} IS NOT NULL AND NEW.{column} NOT IN ({quoted})
BEGIN SELECT RAISE(ABORT, 'Invalid {column}. Must be one of: {label}'); END;
'''

# Always enforce these
print(trigger_sql('status', cfg.get('statuses', ['To Do', 'In Progress', 'Done'])))
print(trigger_sql('priority', cfg.get('priorities', ['Highest', 'High', 'Medium', 'Low', 'Lowest'])))
print(trigger_sql('closed_reason', cfg.get('closed_reasons', ['completed', 'expired', 'wont_do', 'duplicate'])))

# Only enforce if configured
domains = cfg.get('domains', [])
if domains:
    print(trigger_sql('domain', domains))

task_types = cfg.get('task_types', [])
if task_types:
    print(trigger_sql('task_type', task_types))
" "$config"
}

# ── Commands ─────────────────────────────────────────────────────────

cmd_path() {
  echo "$DB_PATH"
}

cmd_config() {
  local key="${1:-}"
  if [[ -z "$key" ]]; then
    read_config_json
  else
    read_config_key "$key"
  fi
}

cmd_validate() {
  validate_config
  echo "Config is valid ($(resolve_config))."
}

cmd_init() {
  mkdir -p "$DB_DIR"

  if [[ -f "$DB_PATH" ]]; then
    echo "Database already exists at $DB_PATH"
    echo "Use 'tusk init --force' to recreate (existing data will be lost)."
    [[ "${1:-}" == "--force" ]] || exit 0
    echo "Recreating database..."
    rm "$DB_PATH"
  fi

  # Copy default config if no project config exists
  if [[ ! -f "$PROJECT_CONFIG" ]]; then
    cp "$DEFAULT_CONFIG" "$PROJECT_CONFIG"
    echo "Created config at $PROJECT_CONFIG — edit to set your project's domains and agents."
  fi

  # Validate config before proceeding
  validate_config

  # Base schema
  sqlite3 "$DB_PATH" <<'SCHEMA'
-- tasks
CREATE TABLE tasks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    summary TEXT NOT NULL,
    description TEXT,
    status TEXT DEFAULT 'To Do',
    priority TEXT DEFAULT 'Medium',
    domain TEXT,
    assignee TEXT,
    task_type TEXT,
    priority_score INTEGER DEFAULT 0,
    github_pr TEXT,
    expires_at TEXT,
    closed_reason TEXT,
    -- Timestamps are UTC (SQLite datetime('now') returns UTC)
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now'))
);

-- task_dependencies
CREATE TABLE task_dependencies (
    task_id INTEGER NOT NULL,
    depends_on_id INTEGER NOT NULL,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (task_id, depends_on_id),
    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE,
    FOREIGN KEY (depends_on_id) REFERENCES tasks(id) ON DELETE CASCADE,
    CHECK (task_id != depends_on_id)
);
CREATE INDEX idx_task_dependencies_task_id ON task_dependencies(task_id);
CREATE INDEX idx_task_dependencies_depends_on_id ON task_dependencies(depends_on_id);

-- task_progress (checkpoint log for context recovery)
CREATE TABLE task_progress (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    task_id INTEGER NOT NULL,
    commit_hash TEXT,
    commit_message TEXT,
    files_changed TEXT,
    next_steps TEXT,
    created_at TEXT DEFAULT (datetime('now')),
    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE
);
CREATE INDEX idx_task_progress_task_id ON task_progress(task_id);

-- task_sessions (metrics)
CREATE TABLE task_sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    task_id INTEGER NOT NULL,
    started_at TEXT NOT NULL,
    ended_at TEXT,
    duration_seconds INTEGER,
    cost_dollars REAL,
    tokens_in INTEGER,
    tokens_out INTEGER,
    lines_added INTEGER,
    lines_removed INTEGER,
    model TEXT,
    FOREIGN KEY (task_id) REFERENCES tasks(id)
);

-- task_metrics view
CREATE VIEW task_metrics AS
SELECT t.*,
    COUNT(s.id) as session_count,
    SUM(s.duration_seconds) as total_duration_seconds,
    SUM(s.cost_dollars) as total_cost,
    SUM(s.tokens_in) as total_tokens_in,
    SUM(s.tokens_out) as total_tokens_out,
    SUM(s.lines_added) as total_lines_added,
    SUM(s.lines_removed) as total_lines_removed
FROM tasks t
LEFT JOIN task_sessions s ON t.id = s.task_id
GROUP BY t.id;
SCHEMA

  # Enable WAL for concurrent read/write support
  sqlite3 "$DB_PATH" "PRAGMA journal_mode = WAL;"

  # Apply validation triggers from config
  local triggers
  triggers="$(generate_triggers)"
  if [[ -n "$triggers" ]]; then
    sqlite3 "$DB_PATH" "$triggers"
  fi

  # Set schema version so fresh DBs never need migration
  sqlite3 "$DB_PATH" "PRAGMA user_version = 2;"

  echo "Initialized task database at $DB_PATH"
}

cmd_regen_triggers() {
  if [[ ! -f "$DB_PATH" ]]; then
    echo "No database found at $DB_PATH — run 'tusk init' first." >&2
    exit 1
  fi

  # Validate config before regenerating triggers
  validate_config

  # Drop all existing validation triggers
  local existing
  existing="$(sqlite3 "$DB_PATH" "SELECT name FROM sqlite_master WHERE type = 'trigger' AND name LIKE 'validate_%';")"
  if [[ -n "$existing" ]]; then
    local drop_sql=""
    while IFS= read -r trig; do
      drop_sql+="DROP TRIGGER IF EXISTS $trig;"
    done <<< "$existing"
    sqlite3 "$DB_PATH" "$drop_sql"
  fi

  # Regenerate and apply from current config
  local triggers
  triggers="$(generate_triggers)"
  if [[ -n "$triggers" ]]; then
    sqlite3 "$DB_PATH" "$triggers"
  fi

  echo "Validation triggers regenerated from config."
}

cmd_shell() {
  exec sqlite3 -header -column -cmd "PRAGMA foreign_keys = ON;" "$DB_PATH"
}

cmd_query() {
  local flags=()
  local sql=""
  for arg in "$@"; do
    if [[ -z "$sql" && "$arg" == -* ]]; then
      flags+=("$arg")
    else
      sql="$arg"
    fi
  done
  sql="PRAGMA foreign_keys = ON; $sql"
  if [[ ${#flags[@]} -gt 0 ]]; then
    exec sqlite3 "${flags[@]}" "$DB_PATH" "$sql"
  else
    exec sqlite3 "$DB_PATH" "$sql"
  fi
}

cmd_version() {
  echo "tusk version $TUSK_VERSION"
}

cmd_migrate() {
  if [[ ! -f "$DB_PATH" ]]; then
    echo "No database found at $DB_PATH — run 'tusk init' first." >&2
    exit 1
  fi

  local current
  current="$(sqlite3 "$DB_PATH" "PRAGMA user_version;")"

  # Migration 0→1: add model column to task_sessions if missing
  if [[ "$current" -lt 1 ]]; then
    local has_model
    has_model="$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM pragma_table_info('task_sessions') WHERE name = 'model';")"
    if [[ "$has_model" -eq 0 ]]; then
      sqlite3 "$DB_PATH" "
        ALTER TABLE task_sessions ADD COLUMN model TEXT;
        PRAGMA user_version = 1;
      "
      echo "  Migration 1: added 'model' column to task_sessions"
    else
      sqlite3 "$DB_PATH" "PRAGMA user_version = 1;"
    fi
  fi

  # Migration 1→2: add task_progress table if missing
  if [[ "$current" -lt 2 ]]; then
    local has_progress
    has_progress="$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='task_progress';")"
    if [[ "$has_progress" -eq 0 ]]; then
      sqlite3 "$DB_PATH" "
        CREATE TABLE task_progress (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            task_id INTEGER NOT NULL,
            commit_hash TEXT,
            commit_message TEXT,
            files_changed TEXT,
            next_steps TEXT,
            created_at TEXT DEFAULT (datetime('now')),
            FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE
        );
        CREATE INDEX idx_task_progress_task_id ON task_progress(task_id);
        PRAGMA user_version = 2;
      "
      echo "  Migration 2: created 'task_progress' table"
    else
      sqlite3 "$DB_PATH" "PRAGMA user_version = 2;"
    fi
  fi

  # Future migrations follow the same pattern:
  # if [[ "$current" -lt 3 ]]; then ... ; sqlite3 "$DB_PATH" "PRAGMA user_version = 3;"; fi

  local final
  final="$(sqlite3 "$DB_PATH" "PRAGMA user_version;")"
  if [[ "$final" -eq "$current" ]]; then
    echo "Schema is up to date (version $final)."
  else
    echo "Migrated schema from version $current → $final."
  fi
}

cmd_upgrade() {
  echo "Checking for updates..."

  # Get latest release tag from GitHub Releases API
  local latest_tag
  latest_tag="$(curl -fsSL "https://api.github.com/repos/$TUSK_GITHUB_REPO/releases/latest" 2>/dev/null \
    | python3 -c "import sys,json; print(json.load(sys.stdin)['tag_name'])")" || {
    echo "Error: Could not fetch latest release from GitHub." >&2
    exit 1
  }

  local remote_version
  remote_version="$(curl -fsSL "https://raw.githubusercontent.com/$TUSK_GITHUB_REPO/refs/tags/$latest_tag/VERSION" 2>/dev/null)" || {
    echo "Error: Could not fetch remote version from GitHub." >&2
    exit 1
  }
  remote_version="$(echo "$remote_version" | tr -d '[:space:]')"

  if [[ "$TUSK_VERSION" -ge "$remote_version" ]]; then
    echo "Already up to date (version $TUSK_VERSION)."
    return
  fi

  echo "Upgrading from version $TUSK_VERSION → $remote_version..."

  tmpdir="$(mktemp -d)"
  trap 'rm -rf "$tmpdir"' EXIT

  # Download and extract tagged release tarball
  curl -fsSL "https://github.com/$TUSK_GITHUB_REPO/archive/refs/tags/$latest_tag.tar.gz" \
    -o "$tmpdir/tusk.tar.gz" || {
    echo "Error: Could not download release tarball." >&2
    exit 1
  }
  tar -xzf "$tmpdir/tusk.tar.gz" -C "$tmpdir"

  # Handle dynamic directory name (tusk-v2, tusk-v3, etc.)
  local src
  src="$(ls -d "$tmpdir"/tusk-*/ 2>/dev/null | head -1)"
  if [[ -z "$src" || ! -d "$src" ]]; then
    echo "Error: Unexpected archive structure." >&2
    exit 1
  fi

  # Copy bin files alongside current binary
  # Use mv (atomic rename) for the running script so bash's open fd keeps
  # reading the old inode instead of seeing overwritten content mid-execution.
  cp "$src/bin/tusk" "$SCRIPT_DIR/tusk.tmp"
  chmod +x "$SCRIPT_DIR/tusk.tmp"
  mv "$SCRIPT_DIR/tusk.tmp" "$SCRIPT_DIR/tusk"
  for pyfile in "$src"/bin/tusk-*.py; do
    [[ -f "$pyfile" ]] || continue
    cp "$pyfile" "$SCRIPT_DIR/"
  done
  cp "$src/VERSION" "$SCRIPT_DIR/VERSION"
  cp "$src/config.default.json" "$SCRIPT_DIR/config.default.json"
  echo "  Updated CLI and support files"

  # Copy skills
  for skill_dir in "$src"/skills/*/; do
    [[ -d "$skill_dir" ]] || continue
    local skill_name
    skill_name="$(basename "$skill_dir")"
    mkdir -p "$REPO_ROOT/.claude/skills/$skill_name"
    cp "$skill_dir"* "$REPO_ROOT/.claude/skills/$skill_name/" 2>/dev/null || true
    echo "  Updated skill: $skill_name"
  done

  # Copy scripts
  mkdir -p "$REPO_ROOT/scripts"
  for script in "$src"/scripts/*.py; do
    [[ -f "$script" ]] || continue
    cp "$script" "$REPO_ROOT/scripts/"
    echo "  Updated scripts/$(basename "$script")"
  done

  # Run migrations using the NEW binary
  "$SCRIPT_DIR/tusk" migrate

  echo ""
  echo "Upgrade complete (version $remote_version)."
}

# ── Dispatch ─────────────────────────────────────────────────────────

case "${1:-}" in
  init)   shift; cmd_init "$@" ;;
  path)   cmd_path ;;
  config) shift; cmd_config "$@" ;;
  validate) cmd_validate ;;
  sql-quote) shift; sql_quote "${1:-}" ;;
  dupes)  shift; exec python3 "$SCRIPT_DIR/tusk-dupes.py" "$DB_PATH" "$(resolve_config)" "$@" ;;
  dashboard) shift; exec python3 "$SCRIPT_DIR/tusk-dashboard.py" "$DB_PATH" "$(resolve_config)" "$@" ;;
  session-stats) shift; exec python3 "$SCRIPT_DIR/tusk-session-stats.py" "$DB_PATH" "$(resolve_config)" "$@" ;;
  shell)  cmd_shell ;;
  version) cmd_version ;;
  migrate) cmd_migrate ;;
  regen-triggers) cmd_regen_triggers ;;
  upgrade) cmd_upgrade ;;
  "")     echo "Usage: tusk {init|path|config|validate|sql-quote|dupes|dashboard|session-stats|shell|version|migrate|upgrade|regen-triggers|\"SQL ...\"}" >&2; exit 1 ;;
  *)      cmd_query "$@" ;;
esac
