#!/usr/bin/env python3
"""Token and cost tracking for tusk task sessions.

Parses Claude Code JSONL transcripts, aggregates token usage per session,
and updates the task_sessions table with tokens_in, tokens_out, cost_dollars,
and model.

Called by the tusk wrapper:
    tusk session-stats <session_id> [transcript_path]

Arguments received from tusk:
    sys.argv[1] — DB path
    sys.argv[2] — config path
    sys.argv[3:] — session_id + optional transcript path
"""

import json
import logging
import os
import sqlite3
import sys
from datetime import datetime, timezone
from pathlib import Path

log = logging.getLogger(__name__)

# Per-million-token pricing (USD) and model aliases are loaded from
# pricing.json at runtime. See load_pricing() below.
PRICING: dict = {}
MODEL_ALIASES: dict = {}


def load_pricing() -> None:
    """Load model pricing and aliases from pricing.json.

    Searches next to this script first (installed layout), then the
    parent directory (source repo layout where pricing.json is at the root).
    """
    global PRICING, MODEL_ALIASES
    script_dir = Path(__file__).resolve().parent
    candidates = [
        script_dir / "pricing.json",
        script_dir.parent / "pricing.json",
    ]
    for path in candidates:
        if path.is_file():
            log.debug("Loading pricing from %s", path)
            with open(path) as f:
                data = json.load(f)
            PRICING = data.get("models", {})
            MODEL_ALIASES = data.get("aliases", {})
            return
    print(
        f"Warning: pricing.json not found (searched {', '.join(str(p) for p in candidates)}). "
        "Cost calculations will return $0.",
        file=sys.stderr,
    )


def resolve_model(model_id: str) -> str:
    """Normalize a model ID to a canonical pricing key."""
    if model_id in PRICING:
        return model_id
    if model_id in MODEL_ALIASES:
        resolved = MODEL_ALIASES[model_id]
        log.debug("Model alias: %s -> %s", model_id, resolved)
        return resolved
    # Try stripping date suffix (e.g. "claude-opus-4-6-20260101")
    for key in PRICING:
        if model_id.startswith(key):
            log.debug("Model prefix match: %s -> %s", model_id, key)
            return key
    log.debug("Unknown model (no pricing): %s", model_id)
    return model_id


def find_transcript(project_dir: str) -> str | None:
    """Find the most recently modified JSONL in the Claude projects dir."""
    claude_dir = Path.home() / ".claude" / "projects" / project_dir
    log.debug("Looking for transcripts in %s", claude_dir)
    if not claude_dir.is_dir():
        log.debug("Directory does not exist")
        return None
    jsonl_files = list(claude_dir.glob("*.jsonl"))
    log.debug("Found %d JSONL files", len(jsonl_files))
    if not jsonl_files:
        return None
    chosen = str(max(jsonl_files, key=lambda p: p.stat().st_mtime))
    log.debug("Selected transcript: %s", chosen)
    return chosen


def derive_project_hash(cwd: str) -> str:
    """Derive Claude Code's project hash from a directory path.

    Claude Code uses the absolute path with '/' replaced by '-',
    e.g. /Users/foo/myproject -> -Users-foo-myproject
    """
    return cwd.replace("/", "-")


def parse_timestamp(ts: str) -> datetime:
    """Parse an ISO 8601 timestamp, handling both Z and +00:00 suffixes."""
    ts = ts.replace("Z", "+00:00")
    return datetime.fromisoformat(ts)


def parse_sqlite_timestamp(ts: str) -> datetime:
    """Parse a SQLite datetime string (UTC, no timezone info)."""
    return datetime.strptime(ts, "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone.utc)


def aggregate_session(
    transcript_path: str,
    started_at: datetime,
    ended_at: datetime | None,
) -> dict:
    """Parse a JSONL transcript and aggregate tokens within the time window.

    Returns dict with keys: input_tokens, output_tokens,
    cache_creation_input_tokens, cache_read_input_tokens, model, request_count.
    """
    log.debug("Aggregating session from %s", transcript_path)
    log.debug("Time window: %s .. %s", started_at.isoformat(),
              ended_at.isoformat() if ended_at else "now")
    seen_requests: set[str] = set()
    totals = {
        "input_tokens": 0,
        "output_tokens": 0,
        "cache_creation_input_tokens": 0,
        "cache_creation_5m_tokens": 0,
        "cache_creation_1h_tokens": 0,
        "cache_read_input_tokens": 0,
    }
    model_counts: dict[str, int] = {}
    request_count = 0
    lines_read = 0

    with open(transcript_path) as f:
        for line in f:
            lines_read += 1
            line = line.strip()
            if not line:
                continue
            try:
                entry = json.loads(line)
            except json.JSONDecodeError:
                continue

            # Only assistant messages have usage data
            if entry.get("type") != "assistant":
                continue

            # Check timestamp is within session window
            ts_str = entry.get("timestamp")
            if not ts_str:
                continue
            try:
                ts = parse_timestamp(ts_str)
            except (ValueError, TypeError):
                continue

            if ts < started_at:
                continue
            if ended_at and ts > ended_at:
                continue

            # Deduplicate by requestId (streaming produces multiple entries)
            request_id = entry.get("requestId")
            if not request_id:
                continue
            if request_id in seen_requests:
                continue
            seen_requests.add(request_id)
            request_count += 1

            # Extract usage
            message = entry.get("message", {})
            usage = message.get("usage", {})
            if not usage:
                continue

            totals["input_tokens"] += usage.get("input_tokens", 0)
            totals["output_tokens"] += usage.get("output_tokens", 0)
            totals["cache_read_input_tokens"] += usage.get(
                "cache_read_input_tokens", 0
            )

            # Per-tier cache write tokens: prefer the nested cache_creation
            # object (ephemeral_5m_input_tokens / ephemeral_1h_input_tokens).
            # Fall back to assigning all cache_creation_input_tokens to the
            # 5m tier when the nested object is absent (older transcripts).
            cache_creation = usage.get("cache_creation")
            cache_total = usage.get("cache_creation_input_tokens", 0)
            if isinstance(cache_creation, dict):
                tokens_5m = cache_creation.get("ephemeral_5m_input_tokens", 0)
                tokens_1h = cache_creation.get("ephemeral_1h_input_tokens", 0)
                totals["cache_creation_5m_tokens"] += tokens_5m
                totals["cache_creation_1h_tokens"] += tokens_1h
            else:
                totals["cache_creation_5m_tokens"] += cache_total
            totals["cache_creation_input_tokens"] += cache_total

            # Track model usage
            model = message.get("model", "")
            if model:
                model = resolve_model(model)
                model_counts[model] = model_counts.get(model, 0) + 1

    log.debug("Lines read: %d, unique requests: %d, duplicates skipped: %d",
              lines_read, request_count, len(seen_requests) - request_count
              if len(seen_requests) > request_count else 0)
    log.debug("Token totals: %s", totals)
    log.debug("Model counts: %s", model_counts)

    # Determine dominant model
    dominant_model = ""
    if model_counts:
        dominant_model = max(model_counts, key=model_counts.get)
    log.debug("Dominant model: %s", dominant_model)

    return {
        **totals,
        "model": dominant_model,
        "model_counts": model_counts,
        "request_count": request_count,
    }


def compute_cost(totals: dict) -> float:
    """Compute cost in dollars from token totals and model.

    Uses five terms: input, cache_write_5m, cache_write_1h, cache_read, output.
    """
    model = totals.get("model", "")
    rates = PRICING.get(model)
    if not rates:
        log.debug("No pricing for model %r — cost = $0", model)
        return 0.0

    mtok = 1_000_000
    cost = (
        totals["input_tokens"] / mtok * rates["input"]
        + totals["cache_creation_5m_tokens"] / mtok * rates["cache_write_5m"]
        + totals["cache_creation_1h_tokens"] / mtok * rates["cache_write_1h"]
        + totals["cache_read_input_tokens"] / mtok * rates["cache_read"]
        + totals["output_tokens"] / mtok * rates["output"]
    )
    log.debug("Cost breakdown (model=%s): input=%d*$%.2f + cache_write_5m=%d*$%.2f "
              "+ cache_write_1h=%d*$%.2f + cache_read=%d*$%.2f + output=%d*$%.2f = $%.6f",
              model,
              totals["input_tokens"], rates["input"],
              totals["cache_creation_5m_tokens"], rates["cache_write_5m"],
              totals["cache_creation_1h_tokens"], rates["cache_write_1h"],
              totals["cache_read_input_tokens"], rates["cache_read"],
              totals["output_tokens"], rates["output"],
              cost)
    return round(cost, 6)


def main():
    # Extract --debug before manual positional parsing
    argv = sys.argv[1:]
    debug = "--debug" in argv
    if debug:
        argv = [a for a in argv if a != "--debug"]

    logging.basicConfig(
        level=logging.DEBUG if debug else logging.WARNING,
        format="[debug] %(message)s",
        stream=sys.stderr,
    )

    load_pricing()

    if len(argv) < 3:
        print(
            "Usage: tusk session-stats [--debug] <session_id> [transcript_path]",
            file=sys.stderr,
        )
        sys.exit(1)

    db_path = argv[0]
    # argv[1] is config_path (unused here but kept for dispatch consistency)
    session_id = argv[2]

    try:
        session_id = int(session_id)
    except ValueError:
        print(f"Error: session_id must be an integer, got '{argv[2]}'", file=sys.stderr)
        sys.exit(1)

    transcript_path = argv[3] if len(argv) > 3 else None
    log.debug("DB path: %s, session_id: %d, transcript_path: %s",
              db_path, session_id, transcript_path)

    # Read session timestamps from DB
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    row = conn.execute(
        "SELECT started_at, ended_at FROM task_sessions WHERE id = ?",
        (session_id,),
    ).fetchone()

    if not row:
        print(f"Error: No session found with id {session_id}", file=sys.stderr)
        conn.close()
        sys.exit(1)

    started_at = parse_sqlite_timestamp(row["started_at"])
    ended_at = parse_sqlite_timestamp(row["ended_at"]) if row["ended_at"] else None

    # Discover transcript if not provided
    if not transcript_path:
        cwd = os.getcwd()
        project_hash = derive_project_hash(cwd)
        transcript_path = find_transcript(project_hash)
        if not transcript_path:
            print(
                f"Error: No JSONL transcripts found for project hash '{project_hash}'.\n"
                f"Looked in: ~/.claude/projects/{project_hash}/\n"
                "Provide the transcript path explicitly.",
                file=sys.stderr,
            )
            conn.close()
            sys.exit(1)

    if not os.path.isfile(transcript_path):
        print(f"Error: Transcript not found: {transcript_path}", file=sys.stderr)
        conn.close()
        sys.exit(1)

    # Aggregate tokens
    totals = aggregate_session(transcript_path, started_at, ended_at)

    if totals["request_count"] == 0:
        print(
            f"Warning: No assistant messages found in time window "
            f"[{started_at.isoformat()} .. {ended_at.isoformat() if ended_at else 'now'}]",
            file=sys.stderr,
        )
        conn.close()
        sys.exit(0)

    tokens_in = (
        totals["input_tokens"]
        + totals["cache_creation_input_tokens"]
        + totals["cache_read_input_tokens"]
    )
    tokens_out = totals["output_tokens"]
    cost = compute_cost(totals)
    model = totals["model"]

    # Update DB
    conn.execute(
        """UPDATE task_sessions
           SET tokens_in = ?, tokens_out = ?, cost_dollars = ?, model = ?
           WHERE id = ?""",
        (tokens_in, tokens_out, cost, model, session_id),
    )
    conn.commit()
    conn.close()

    # Print summary
    print(f"Session {session_id} token stats updated:")
    print(f"  Model:        {model}")
    print(f"  Requests:     {totals['request_count']}")
    print(f"  Input tokens: {tokens_in:,} (base: {totals['input_tokens']:,}, "
          f"cache write 5m: {totals['cache_creation_5m_tokens']:,}, "
          f"cache write 1h: {totals['cache_creation_1h_tokens']:,}, "
          f"cache read: {totals['cache_read_input_tokens']:,})")
    print(f"  Output tokens: {tokens_out:,}")
    print(f"  Est. cost:    ${cost:.4f}")


if __name__ == "__main__":
    main()
